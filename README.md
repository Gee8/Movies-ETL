# Movies-ETL

## Overview
In this analysis, we have created a function `extract_transform_load()` to read in three files, transform the data within them, and upload them into a PostgreSQL file. The three files taken in were `movies_metadata.csv`, `ratings.csv`, and `wikipedia-movies.json`. First we filtered the wiki_movies data from the .json file to remove all television shows then read the file as a dataframe. From here we removed columns that contained less than 90% NaN values. We then cleaned the box office, budget, release_date and running_time columns using regular expressions. Next we cleaned the kaggle_metadata we got from the `movies_metadata.csv` file. We kept all the rows such that the adult column value was `'False'`, the `video` column contained only `'True'` values, made the `budget` column an int type, used the pandas to_numeric function on the popularity column, and used the to_datetime function to convert the release_date column to a date type. We then merged the `wiki_movies_df` and the `kaggle_metadata` on the `imdb_id` column to create the `movies_df` dataframe. We created a function `fill_missing_kaggle_data` to use the wiki data in place of the kaggle data for instances where the data in the kaggle column was 0, then dropped the wiki data column. We used this function for the running time, budget, and revenue columns. We then filtered the `movies_df` for the specific columns we wanted, then renamed them. Now that our movies data was cleaned, we transformed the ratings data from the `ratings.csv` and merged it with the `movies_df`, creating a `movies_with_ratings_df`. Finally, we created a connection to PostgreSQL to upload our `movies_df` and `ratings` table. Since the ratings table is so large, we imported in chunks of 1,000,000 rows at a time, and printed out the progress for each step including the total amount of time elapsed in seconds.